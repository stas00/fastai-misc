{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml, torch, gc\n",
    "from ipyexperiments import IPyExperimentsPytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, GeForce GTX 1070 Ti (8119 RAM)\n",
      "\n",
      "･ RAM: △Consumed △Peaked  Used Total | Exec time 0.000s\n",
      "･ CPU:         0       0     2110 MB |\n",
      "･ GPU:         0       0     6010 MB |\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(exp_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM: △Consumed △Peaked  Used Total | Exec time 0.001s\n",
      "･ CPU:         0       0     2110 MB |\n",
      "･ GPU:         0       0     6010 MB |\n"
     ]
    }
   ],
   "source": [
    "# light weight humanize from https://stackoverflow.com/a/1094933/9201239 w/ tweaks\n",
    "def hs(num, suffix='B'):\n",
    "    for unit in ['','K','M','G','T','P','E','Z']:\n",
    "        if abs(num) < 1024.0: return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Y', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM: △Consumed △Peaked  Used Total | Exec time 0.051s\n",
      "･ CPU:         0       0     2110 MB |\n",
      "･ GPU:         0       0     6010 MB |\n"
     ]
    }
   ],
   "source": [
    "pynvml.nvmlInit()\n",
    "id = torch.cuda.current_device()\n",
    "def mem_free():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(id)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    return int( info.free / 2**20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def mem_report(): print(f\"free mem={mem_free()}\")\n",
    "\n",
    "def mem_allocate_mbs(n, fatal=False): \n",
    "    \" allocate n MBs, return the var holding it on success, None on failure \"\n",
    "    if n < 6: return None # don't try to allocate less than 6MB\n",
    "    try:\n",
    "        d = int(2**9*n**0.5)\n",
    "        return torch.ones((d, d)).cuda().contiguous()\n",
    "    except Exception as e:\n",
    "        if not fatal: return None\n",
    "        print(f\"allocated={hs(torch.cuda.memory_allocated())}, max allocated={hs(torch.cuda.max_memory_allocated())}, cached={hs(torch.cuda.memory_cached())}, max cached={hs(torch.cuda.max_memory_cached())} \")\n",
    "        raise e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay\n",
      "･ RAM: △Consumed △Peaked  Used Total | Exec time 0.308s\n",
      "･ CPU:         0       0     2110 MB |\n",
      "･ GPU:         0     660     6010 MB |\n"
     ]
    }
   ],
   "source": [
    "def consume_gpu_mbs(n):\n",
    "    d = int(2**9*n**0.5)\n",
    "    return torch.ones((d, d)).cuda()\n",
    "if 'x1' in locals(): del x1\n",
    "x1 = mem_allocate_mbs(660)\n",
    "#x1 = consume_gpu_mbs(10)\n",
    "if x1 is None:\n",
    "    print(\"failed to allocate\")\n",
    "else:\n",
    "    print(\"yay\")\n",
    "del x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM: △Consumed △Peaked  Used Total | Exec time 0.006s\n",
      "･ CPU:         0       0     2110 MB |\n",
      "･ GPU:         0       0     6010 MB |\n"
     ]
    }
   ],
   "source": [
    "exp.cl.data[1].peaked_delta / 2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avail: 2109\n",
      "･ RAM: △Consumed △Peaked  Used Total | Exec time 0.037s\n",
      "･ CPU:         0       0     2110 MB |\n",
      "･ GPU:         0       0     6010 MB |\n"
     ]
    }
   ],
   "source": [
    "avail = mem_free()\n",
    "print(f\"Avail: {avail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free mem=2109\n",
      "free mem=1595\n",
      "free mem=1081\n",
      "free mem=1595\n",
      "free mem=95\n",
      "･ RAM: △Consumed △Peaked  Used Total | Exec time 1.000s\n",
      "･ CPU:         0       0     2111 MB |\n",
      "･ GPU:         0    2014     6010 MB |\n"
     ]
    }
   ],
   "source": [
    "success = []\n",
    "\n",
    "threshold = 10\n",
    "\n",
    "def get(goal, want=0, delta=0, depth=0):\n",
    "    if want  == 0: want  = goal        \n",
    "    if delta == 0: delta = goal\n",
    "        \n",
    "    delta = int(delta/2)\n",
    "\n",
    "    # threshold to stop at\n",
    "    if want  < threshold  : return 0\n",
    "    if delta < threshold/2: return 0\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    x = mem_allocate_mbs(want)\n",
    "    if x is not None: # success, try more\n",
    "        del x\n",
    "        print(f\"yes: {depth} {want}, {delta} ({goal})\")\n",
    "        return max(want, get(goal, want+delta, delta, depth+1))\n",
    "    else:             # failure, try less\n",
    "        print(f\" no: {depth} {want}, {delta} ({goal})\")\n",
    "        return           get(goal, want-delta, delta, depth+1)   \n",
    "\n",
    "\n",
    "def memmap():\n",
    "    last_tried = 0\n",
    "    blocks = []\n",
    "    store  = []\n",
    "    while True:\n",
    "        avail = mem_free()\n",
    "        print(f\"have {avail}\")\n",
    "        if avail < threshold: break\n",
    "        #if avail > last_tried: break\n",
    "        #last_tried = avail\n",
    "        \n",
    "        size = get(avail)\n",
    "        print(f\"wanted to get {avail} got {size}\")\n",
    "        if not size: break\n",
    "        blocks.append(str(size))\n",
    "        store.append(mem_allocate_mbs(size))\n",
    "        print(f\"got block of size {size}\")\n",
    "        \n",
    "    # free the tmp memory\n",
    "    store = []\n",
    "    print(f\"Free blocks in MBs: {', '.join(blocks)}\")\n",
    "\n",
    "          \n",
    "for x in ['x1', 'x2', 'x3']: \n",
    "    if x in locals(): del x \n",
    "\n",
    "gc.collect()\n",
    "\n",
    "mem_report()\n",
    "          \n",
    "# create a hole of 512MB\n",
    "x1 = mem_allocate_mbs(514, fatal=True)\n",
    "mem_report()\n",
    "          \n",
    "x2 = mem_allocate_mbs(514, fatal=True)    \n",
    "mem_report()\n",
    "          \n",
    "del x1\n",
    "mem_report()\n",
    "          \n",
    "x3 = mem_allocate_mbs(1500, fatal=True)    \n",
    "mem_report()\n",
    "          \n",
    "# detect the hole\n",
    "#memmap()\n",
    "# x = get(10000)\n",
    "# print(x)\n",
    "#mem_report()\n",
    "          \n",
    "# cleanup\n",
    "del x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have 2109, want 0\n",
      "have 2109, want 0\n",
      "have 2109, want 128\n",
      "have 1981, want 128\n",
      "have 1981, want 256\n",
      "have 1725, want 256\n",
      "have 1725, want 384\n",
      "have 1341, want 384\n",
      "have 1341, want 512\n",
      "have 829, want 512\n",
      "have 829, want 640\n",
      "have 189, want 640\n",
      "allocated=1.9 GB, max allocated=2.0 GB, cached=1.9 GB, max cached=2.0 GB \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 640.00 MiB (GPU 0; 7.93 GiB total capacity; 1.88 GiB already allocated; 189.56 MiB free; 0 bytes cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dd587c7a3805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem_allocate_mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfatal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"have {mem_free()}, want {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_allocate_mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfatal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-230a1ed801a3>\u001b[0m in \u001b[0;36mmem_allocate_mbs\u001b[0;34m(n, fatal)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfatal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"allocated={hs(torch.cuda.memory_allocated())}, max allocated={hs(torch.cuda.max_memory_allocated())}, cached={hs(torch.cuda.memory_cached())}, max cached={hs(torch.cuda.max_memory_cached())} \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-230a1ed801a3>\u001b[0m in \u001b[0;36mmem_allocate_mbs\u001b[0;34m(n, fatal)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#return torch.ones(int(mbytes)*2**18, 1).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 640.00 MiB (GPU 0; 7.93 GiB total capacity; 1.88 GiB already allocated; 189.56 MiB free; 0 bytes cached)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM: △Consumed △Peaked  Used Total | Exec time 1.569s\n",
      "･ CPU:         0       0     2113 MB |\n",
      "･ GPU:      1920       0     7930 MB |\n"
     ]
    }
   ],
   "source": [
    "stack = []\n",
    "for s in range(8):\n",
    "    size = s*128\n",
    "    print(f\"have {mem_free()}, want {size}\")\n",
    "    x1 = mem_allocate_mbs(size, fatal=True)\n",
    "    print(f\"have {mem_free()}, want {size}\")\n",
    "    stack.append(mem_allocate_mbs(size, fatal=True))\n",
    "    del x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
